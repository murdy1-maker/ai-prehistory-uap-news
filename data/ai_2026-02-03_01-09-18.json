[
  {
    "title": "Why AI adoption keeps outrunning governance \u2014 and what to do about it",
    "link": "https://www.computerworld.com/article/4122948/responsible-ai-gap-why-ai-adoption-keeps-outrunning-governance-and-what-to-do-about-it.html",
    "description": "Across industries, CIOs are rolling out generative AI through SaaS platforms, embedded copilots, and third-party tools at a speed that traditional governance frameworks were never designed to handle. AI now influences customer interactions, hiring decisions, financial analysis, software development, and knowledge work \u2014 often without being formally deployed in the classical sense.The result is a widening gap between rapid AI deployment and responsible-use protections. Organizations adopt AI faster than they can govern its usage, then scramble to retrofit controls after something goes wrong.Interviews with five practitioners \u2014 each working at a different pressure point of enterprise AI \u2014 reveal why this gap persists and what leaders must do to close it before regulators, auditors, or customers force the issue.Why governance breaks the moment AI hits real workflowsThe first problem is structural. Governance was designed for centralized, slow-moving decisions. AI adoption is neither. Ericka Watson, CEO of consultancy Data Strategy Advisors and former chief privacy officer at Regeneron Pharmaceuticals, sees the same pattern across industries.\u201cCompanies still design governance as if decisions moved slowly and centrally,\u201d she said. \u201cBut that\u2019s not how AI is being adopted. Businesses are making decisions daily \u2014 using vendors, copilots, embedded AI features \u2014 while governance assumes someone will stop, fill out a form, and wait for approval.\u201dThat mismatch guarantees bypass. Even teams with good intentions route around governance because it doesn\u2019t appear where work actually happens. AI features go live before anyone assesses training data rights, downstream sharing, or accountability.What breaks first, Watson said, is data control and visibility. Employees paste sensitive information into public genAI tools, and data lineage disappears as outputs move across systems. \u201cBy the time leadership realizes what\u2019s happening,\u201d she said, \u201cthe data may already be gone in ways you can\u2019t undo.\u201dWhat to do: CIOs must move from model governance to usage governance. You may not control the model, but you can control how it\u2019s used, what data it touches, and where outputs flow. Governance has to be embedded as tollgates inside workflows, not in policy documents that are reviewed after the fact.Why legacy data governance collapses under genAIEven where governance exists, it\u2019s often built on assumptions that no longer hold. Fawad Butt, CEO of agentic healthcare platform maker Penguin Ai and former chief data officer at UnitedHealth Group and Kaiser Permanente, argues that traditional data governance models are structurally unfit for generative AI.\u201cClassic governance was built for systems of record and known analytics pipelines,\u201d he said. \u201cThat world is gone. Now you have systems creating systems \u2014 new data, new outputs, and much is done on the fly.\u201d In that environment, point-in-time audits create false confidence. Output-focused controls miss where the real risk lives.\u201cNo breach is required for harm to occur \u2014 secure systems can still hallucinate, discriminate, or drift,\u201d Butt said, emphasizing that inputs, not outputs, are now the most neglected risk surface. This includes prompts, retrieval sources, context, and any tools AI agents can dynamically access.What to do: Before writing policy, establish guardrails. Define no-go use cases. Constrain high-risk inputs. Limit tool access for agents. And observe how systems behave in practice. Policy should come after experimentation, not before. Otherwise, organizations hard-code assumptions that are already wrong.Why vendor AI is where governance collapsesIf internal AI governance is weak, third-party AI governance is worse. Richa Kaul, CEO of Complyance, works with global enterprises on risk and compliance management. She sees a sharp divide: while companies are relatively mature in governing AI they build themselves, they are much less prepared when AI arrives embedded in vendor products.\u201cWhat we\u2019re seeing is use before governance,\u201d she said. \u201cAnd it\u2019s often governance by committee \u2014 10 to 20 people reviewing vendors one by one without a shared baseline of questions.\u201d Too often, enterprises ask open-ended questions about AI privacy and accept reassuring answers \u2014 what Kaul calls \u201chappy ears.\u201dMature governance shows up in specific questions. Is customer data used to train models? Is it reused across clients? Is the LLM accessed via an enterprise deployment or a consumer interface?\u201cA vendor using Azure OpenAI has a much lower risk profile than one calling ChatGPT directly,\u201d Kaul said.What to do: CIOs should start with a basic but overlooked step: scrutinize vendor subprocessor lists. Cloud providers are well understood. LLM providers are not. AI has created a second, poorly mapped subprocessor layer \u2014 and that\u2019s where governance breaks down.Why bans fail and incidents repeatTechnology controls alone do not close the responsible-AI gap. Behavior matters more. Asha Palmer, SVP of Compliance Solutions at Skillsoft and a former US federal prosecutor, is often called in after AI incidents. She says the first uncomfortable truth leaders confront is that the outcome was predictable.\u201cWe knew this could happen,\u201d she said. \u201cThe real question is: why didn\u2019t we equip people to deal with it before it did?\u201d Pressure to perform is the root cause. Employees use AI to move faster and meet targets \u2014 just as they have in every compliance failure from bribery to data misuse.Blanket bans on genAI do not work. \u201cIf you take away responsible use,\u201d Palmer said, \u201cpeople will use it irresponsibly \u2014 in secret, in ways you can\u2019t govern.\u201dWhat to do: Shift from awareness training to behavioral learning. Palmer calls it \u201cmoral muscle memory,\u201d a scenario-based practice that teaches people to stop, assess risk, and choose a better action under pressure.Regulators and auditors look for evidence that the right people have received the right training for the risks they actually face. One-size-fits-all AI literacy is a red flag.Why confidence is not enough when auditors arriveThe final gap appears when organizations are asked to prove their governance works. Danny Manimbo is ISO & AI Practice Leader at Schellman, an attestation and compliance services provider. He sees the same failure pattern repeatedly.\u201cOrganizations confuse having policies with having governance,\u201d he said. \u201cResponsible AI principles don\u2019t matter if they don\u2019t influence real decisions.\u201dAuditors might start with a simple request: show us a documented AI risk-based decision that changed an outcome. Mature governance leaves fingerprints \u2014 including delayed deployments, rejected vendors, and constrained features. Immature governance produces vague assurances.\u201cThe most expensive governance work is the work you try to do after deployment,\u201d Manimbo warned. Walking back data lineage, accountability, and intended purpose is extraordinarily difficult once systems are live.What to do: Treat AI governance as a management system, not a compliance exercise. Standards like ISO/IEC 42001 work only when they connect risk management, change control, monitoring, and internal audit into a continuous loop.You can tell governance is working when it changes business decisions, not when it produces documentation.Closing the responsible AI gapAcross all five interviews, one theme recurs: the responsible AI gap is not primarily a technology failure. It\u2019s a governance timing failure. Controls are being designed for yesterday\u2019s systems while AI is already shaping today\u2019s decisions.Several of the sources stressed that CIOs should stop framing responsible AI as a future-state program and start treating it as an operational hygiene issue \u2014 closer to identity management or financial controls than to ethics committees.Watson from Data Strategy Advisors emphasized that visibility is the first non-negotiable step. Enterprises that cannot enumerate where AI influences decisions \u2014 especially through SaaS tools \u2014 are already exposed. \u201cYou can\u2019t govern what you can\u2019t see,\u201d she noted, warning that many companies still lack even a basic inventory of AI-affected workflows.At Penguin Ai, Butt reinforced that point from a data perspective, arguing that inventories must shift from platforms to systems-in-context. An AI feature embedded in HR software and the same feature embedded in marketing automation do not carry the same risk. Treating them as identical is a governance illusion.Complyance\u2019s Kaul added that the same principle applies externally. Vendor AI governance breaks down when enterprises accept generic assurances instead of mapping where their data actually flows. In her experience, simply forcing teams to trace AI subprocessors exposes risks that executives did not realize they had accepted.To close the gap, CIOs must:>Embed governance where work happens \u2014 and before it happens, not after.Shift focus from models to usage and inputs.Treat vendor AI as a first-class risk domain.Replace bans with behavioral training.Demand governance that provides explanations as to how decisions are made.The organizations that do these things will not only avoid regulatory trouble, they will move faster \u2014 and with more confidence.Palmer from Skillsoft focused on the human layer that sits underneath all of this. Governance frameworks collapse, she argued, when they assume people will slow down under pressure. \u201cPressure doesn\u2019t disappear,\u201d she said. \u201cYou have to train for it.\u201d Organizations that fail to do so should not be surprised when employees improvise with AI in unsafe ways.Finally, Schellman\u2019s Manimbo offered a blunt litmus test: if governance has never delayed a deployment, rejected a vendor, or constrained a feature, it probably does not exist in practice. \u201cGovernance has to leave fingerprints,\u201d he said. Otherwise, it is indistinguishable from aspiration.Taken together, the interviews suggest that closing the responsible AI gap does not require perfect foresight or exhaustive policy. It requires earlier intervention and clearer accountability. Organizations that act now \u2014 while AI use is still fragmented and informal \u2014 have a chance to shape behavior. Those that wait will inherit systems they no longer control and risks they can no longer explain.At that point, governance is no longer a choice. It becomes damage control.Related reading:GenAI in productivity apps: What could possibly go wrong?Overcome governance and trust issues to drive agentic AIThe trick to balancing governance with innovation in the age of AIDeloitte\u2019s AI governance failure exposes critical gap in enterprise quality controls",
    "source_id": "computerworld_in",
    "pubDate": "2026-02-02 13:07:20"
  },
  {
    "title": "NVIDIA Just Made Another Smart Bet on AI",
    "link": "https://247wallst.com/investing/2026/02/02/nvidia-just-made-another-smart-bet-on-ai/",
    "description": "Nvidia (NASDAQ:NVDA) just seems to keep the deals and partnerships coming. Undoubtedly, the GPU king has plenty of cash to spend and, perhaps most importantly, efficiencies to unlock as it opts to team up to tackle some of the world\u2019s greatest challenges. At the end of the day, more \u201ccircular\u201d dealmaking may be a growing ... NVIDIA Just Made Another Smart Bet on AIThe post NVIDIA Just Made Another Smart Bet on AI appeared first on 24/7 Wall St..",
    "source_id": "wallst_247",
    "pubDate": "2026-02-02 13:07:12"
  },
  {
    "title": "GCC School and KC GlobEd Host Round Table Meeting on Impact of AI on GCCs \u2013 Building the Future Workforce",
    "link": "https://www.newsx.com/business-news/gcc-school-and-kc-globed-host-round-table-meeting-on-impact-of-ai-on-gccs-building-the-future-workforce-158009/",
    "description": "You Might Be Interested In After 12 Years of DIVORCE, Is Jennifer Winget Marrying Again? Wedding Rumors, Karan Wahi\u2019s Ex-Girlfriend Secrets & Personal Details Exposed Kannada TV Star...",
    "source_id": "newsx",
    "pubDate": "2026-02-02 13:07:10"
  },
  {
    "title": "CloudForge Secures $3.95 Million to Bring AI to the Metals Supply Chain",
    "link": "https://www.businesswire.com/news/home/20260202589324/en",
    "description": "NEW YORK--(BUSINESS WIRE)--CloudForge, the AI-powered sales and procurement platform purpose-built for the metals supply chain, today announced it has emerged from stealth and raised $3.95 million in funding from Zero Infinity Partners, Resolute Ventures, and Bienville Capital. Metal is the backbone of American manufacturing, powering aerospace, defense, automotive, energy, construction, medical devices, and industrial production. Yet the businesses that move metal every day (metal service cent",
    "source_id": "businesswire",
    "pubDate": "2026-02-02 13:07:00"
  },
  {
    "title": "Wealthy Families Aren\u2019t Investing Enough in AI. A Shift Is Coming",
    "link": "https://biztoc.com/x/0d4cf13ff7838447",
    "description": null,
    "source_id": "biztoc",
    "pubDate": "2026-02-02 13:04:52"
  }
]